# %% [markdown]
# # 03 - Modelo con preprocesado y SVM
# Este notebook entrena un SVM usando los datos preprocesados. 
# - Busca archivos preprocesados en el directorio del repo.
# - Si no encuentra ninguno, intenta cargar `data.csv` o `dataset.csv`.
# - Realiza split, estandariza y entrena SVM con GridSearchCV.
# - Guarda el modelo y muestra métricas.

# %% 
# Imports básicos
import os
import glob
import pickle
from pathlib import Path

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, f1_score, classification_report, confusion_matrix,
    roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt

# %% [markdown]
# ## 1) Encontrar y cargar datos preprocesados
# Buscamos archivos típicos generados por notebooks de preprocesado: csv, parquet, npy o pkl que contengan "preproc" o "preprocesado".
# Si no los encuentra, intenta cargar archivos comunes `data.csv`, `dataset.csv`.

# %%
def find_preprocessed_file():
    patterns = [
        "*preproc*.csv", "*preprocessed*.csv", "*preprocesado*.csv",
        "*preproc*.parquet", "*preprocessed*.parquet", "*preprocesado*.parquet",
        "*preproc*.pkl", "*preprocessed*.pkl", "*preprocesado*.pkl",
        "*preproc*.npz", "*preproc*.npy"
    ]
    for p in patterns:
        for f in glob.glob(p):
            return f
    # buscar csv comunes
    for f in ("data.csv", "dataset.csv", "datos.csv"):
        if os.path.exists(f):
            return f
    return None

pre_file = find_preprocessed_file()
print("Archivo preprocesado encontrado:", pre_file)

# %%
if pre_file is None:
    raise FileNotFoundError(
        "No se encontró archivo preprocesado ni data.csv/dataset.csv en el directorio. "
        "Si tu preprocesado está en otro lugar, indícame la ruta o guarda el CSV con un nombre común."
    )

# Cargar según extensión
ext = Path(pre_file).suffix.lower()
if ext in (".csv",):
    df = pd.read_csv(pre_file)
elif ext in (".parquet",):
    df = pd.read_parquet(pre_file)
elif ext in (".pkl", ".joblib"):
    df = pd.read_pickle(pre_file)
elif ext in (".npy", ".npz"):
    arr = np.load(pre_file, allow_pickle=True)
    # Si es array 2d -> crear df simple con columnas genéricas
    if isinstance(arr, np.ndarray):
        df = pd.DataFrame(arr)
    else:
        # .npz con items
        try:
            df = pd.DataFrame(arr['arr_0'])
        except Exception:
            raise RuntimeError("Formato npz no esperado.")
else:
    raise RuntimeError(f"Extensión {ext} no soportada automáticamente.")

print("Dimensiones del dataframe:", df.shape)
df.head()

# %% [markdown]
# ## 2) Preparar X e y
# Asumimos que la columna objetivo se llama `target`, `label` o `y`. Si tiene otro nombre, modifica `target_col`.

# %%
possible_targets = ['target','label','y','clase','Clase','TARGET']
target_col = None
for t in possible_targets:
    if t in df.columns:
        target_col = t
        break

if target_col is None:
    # si el dataframe tiene muchas columnas, proponemos la última como objetivo
    target_col = df.columns[-1]
    print("No se detectó columna objetivo típica; usaré la última columna como target:", target_col)

X = df.drop(columns=[target_col])
y = df[target_col]

print("X shape:", X.shape, "y shape:", y.shape)
print("Valores únicos target:", np.unique(y)[:10])

# %% [markdown]
# ## 3) Split train/test
# Stratified split si la variable es categórica

# %%
test_size = 0.2
random_state = 42

if len(np.unique(y)) == 2:
    strat = y
else:
    strat = y

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=random_state, stratify=strat
)

print("Train:", X_train.shape, "Test:", X_test.shape)

# %% [markdown]
# ## 4) Pipeline con StandardScaler + SVM y búsqueda de hiperparámetros
# SVM con kernel RBF por defecto — se puede cambiar.

# %%
pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("svc", SVC(probability=True, random_state=random_state))
])

param_grid = {
    "svc__C": [0.1, 1, 10],
    "svc__gamma": ["scale", "auto", 0.01, 0.1],
    "svc__kernel": ["rbf"]  # si quieres probar linear, añadir "linear"
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)

grid = GridSearchCV(pipe, param_grid, cv=cv, scoring="f1", n_jobs=-1, verbose=2)

# Entrenar
grid.fit(X_train, y_train)

print("Mejor score (CV):", grid.best_score_)
print("Mejores parámetros:", grid.best_params_)

# %% [markdown]
# ## 5) Evaluación en test

# %%
best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)

acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='binary' if len(np.unique(y))==2 else 'weighted')
report = classification_report(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("Accuracy:", acc)
print("F1:", f1)
print("Classification report:\n", report)
print("Confusion matrix:\n", cm)

# %% [markdown]
# ### Curva ROC (si problema binario)

# %%
if len(np.unique(y)) == 2:
    y_proba = best_model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_proba)
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    print("ROC AUC:", auc)
    plt.figure()
    plt.plot(fpr, tpr, label=f"AUC = {auc:.3f}")
    plt.plot([0,1],[0,1], linestyle="--")
    plt.xlabel("FPR")
    plt.ylabel("TPR")
    plt.title("ROC Curve - Modelo 03 (SVM)")
    plt.legend()
    plt.grid(True)
    plt.show()

# %% [markdown]
# ## 6) Guardar modelo y resultados

# %%
os.makedirs("outputs", exist_ok=True)
model_path = os.path.join("outputs", "modelo_03_svm.pkl")
with open(model_path, "wb") as f:
    pickle.dump(best_model, f)

metrics = {
    "accuracy": float(acc),
    "f1": float(f1),
    "best_params": grid.best_params_
}
pd.DataFrame([metrics]).to_csv("outputs/metrics_modelo_03_svm.csv", index=False)
print("Modelo guardado en:", model_path)
print("Métricas guardadas en: outputs/metrics_modelo_03_svm.csv")

# %% [markdown]
# ## 7) Notas y siguientes pasos
# - Si tu notebook de preprocesado (`/mnt/data/02 - preprocesado.ipynb`) exporta un archivo con nombre concreto (p. ej. `X_preproc.csv` o `X_train.npy`), renombra o mueve ese archivo al directorio donde corres este notebook para que sea detectado automáticamente.
# - Puedes cambiar la `param_grid` para ajustar más combinaciones (C más grandes, kernels lineales).
# - Para problemas multiclase ajusta `scoring` en GridSearchCV (por ejemplo `f1_weighted`) y la métrica de evaluación.
